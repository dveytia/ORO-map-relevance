{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9092aa5e-921f-4134-af31-a801356e83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313964a3-88d8-4dd6-9ec8-e8ec537cf836",
   "metadata": {},
   "source": [
    "# Make functions to extract best model score\n",
    "\n",
    "Because the model scores have different numbers of columns depending on if it is single label or multi label, format two different types of functions. Test them out on an example to make sure they produce similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0195b48-42f0-4d45-9ae8-56f6ea2bfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_single(modelname, file_path_prefix, k_range):\n",
    "    inner_scores = []\n",
    "    params = ['batch_size','weight_decay','learning_rate','num_epochs','class_weight']\n",
    "    \n",
    "    for k in range(k_range): #Change to 5 if you are using the binary\n",
    "        inner_df = pd.read_csv(f'{file_path_prefix}{k}.csv')\n",
    "        inner_df = inner_df.sort_values('F1',ascending=False).reset_index(drop=True)\n",
    "        inner_scores += inner_df.to_dict('records')\n",
    "    \n",
    "    inner_scores = pd.DataFrame.from_dict(inner_scores).fillna(-1)\n",
    "    best_model = (inner_scores\n",
    "                  .groupby(params).agg({\n",
    "                      'F1':'mean',\n",
    "                      'ROC AUC':'mean',\n",
    "                      'precision':'mean',\n",
    "                      'recall':'mean',\n",
    "                      'accuracy':'mean'\n",
    "                      }).sort_values('F1',ascending=False).reset_index()).to_dict('records')[0]\n",
    "    best_model = pd.DataFrame(best_model,index=[modelname])\n",
    "    \n",
    "    # re-arrange so hyperparameters are at the end\n",
    "    best_model = best_model[[c for c in best_model if c not in params] \n",
    "        + [c for c in params if c in best_model]]\n",
    "    \n",
    "    del inner_scores, inner_df\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab2527e8-a218-411d-942c-f1508b2e3125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>class_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>climate_mitigation</th>\n",
       "      <td>0.802477</td>\n",
       "      <td>0.938158</td>\n",
       "      <td>0.726099</td>\n",
       "      <td>0.897321</td>\n",
       "      <td>0.879555</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          F1   ROC AUC  precision    recall  accuracy  \\\n",
       "climate_mitigation  0.802477  0.938158   0.726099  0.897321  0.879555   \n",
       "\n",
       "                    batch_size  weight_decay  learning_rate  num_epochs  \\\n",
       "climate_mitigation          16           0.0        0.00001           2   \n",
       "\n",
       "                    class_weight  \n",
       "climate_mitigation            -1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1=get_best_model_single('climate_mitigation',f'/home/dveytia/ORO-map-relevance/outputs/model_selection/climate_mitigation_model_selection_', 3)\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185b7857-0623-4c2c-8870-4988d10f7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_multi(modelname, file_path_prefix, k_range): \n",
    "    inner_scores = []\n",
    "    params = ['batch_size','weight_decay','learning_rate','num_epochs','class_weight']\n",
    "    \n",
    "    for k in range(k_range): \n",
    "        inner_df = pd.read_csv(f'{file_path_prefix}{k}.csv')\n",
    "        inner_df = inner_df.sort_values('F1 macro', ascending=False).reset_index(drop=True)\n",
    "        inner_scores += inner_df.to_dict('records')\n",
    "    \n",
    "    inner_scores = pd.DataFrame.from_dict(inner_scores).fillna(-1)\n",
    "    \n",
    "    #if 'accuracy macro' not in list(inner_scores.columns): # if there is no accuracy macro column set to dummy value\n",
    "    #    inner_scores['accuracy macro'] = -999 \n",
    "    best_model = (inner_scores\n",
    "                .groupby(params).agg({\n",
    "                    'F1 macro':'mean',\n",
    "                    'F1 micro':'mean',\n",
    "                    'F1 weighted':'mean',\n",
    "                    'ROC AUC macro':'mean',\n",
    "                    'precision macro':'mean',\n",
    "                    'recall macro':'mean'\n",
    "                    }).sort_values('F1 macro',ascending=False).reset_index()).to_dict('records')[0]\n",
    "    \n",
    "    best_model = pd.DataFrame(best_model,index=[modelname])\n",
    "    #best_model.rename(columns={'F1 macro': 'F1', 'ROC AUC macro': 'ROC AUC', 'precision macro':'precision', 'recall macro':'recall', 'accuracy macro':'accuracy'}, inplace=True)\n",
    "    \n",
    "    \n",
    "    ## Get label information\n",
    "    allLabels = [x for x in inner_scores.columns if 'F1' in x] \n",
    "    allLabels.remove('F1 macro')\n",
    "    allLabels.remove('F1 micro')\n",
    "    allLabels.remove('F1 weighted')\n",
    "    allLabels.remove('F1 samples')\n",
    "    allLabels = [s.replace('F1 - ', '') for s in allLabels]\n",
    "    \n",
    "    #best_model['label_names'] = \"temp\"\n",
    "    #best_model.at[modelname,'label_names'] = [s.replace(modelname + '.','') for s in allLabels]\n",
    "    \n",
    "    \n",
    "    ## Get label F1s\n",
    "    #labelScores = []\n",
    "    #for label in allLabels:\n",
    "    #    best_model_temp = (inner_scores\n",
    "    #              .groupby(params).agg({\n",
    "    #                  'F1'+' - ' + label:'mean'\n",
    "    #                  }).sort_values('F1'+' - ' + label,ascending=False).reset_index()).to_dict('records')[0]\n",
    "    #    best_model_temp = pd.DataFrame(best_model_temp, index=[modelname])\n",
    "    #    labelScores.append(best_model_temp['F1 - '+label])\n",
    "    #    \n",
    "    #best_model['label_F1s'] = \"temp\"\n",
    "    #best_model.at[modelname,'label_F1s'] = [ '%.2f' % elem for elem in labelScores]\n",
    "    \n",
    "    \n",
    "    ## Start loop to get label scores\n",
    "    for label in allLabels:\n",
    "        # get the best model\n",
    "        best_model_temp = (inner_scores\n",
    "                           .groupby(params).agg({\n",
    "                               'F1'+' - ' + label:'mean',\n",
    "                               'ROC AUC'+' - ' + label:'mean',\n",
    "                               'precision'+' - ' + label:'mean',\n",
    "                               'recall'+' - ' + label:'mean',\n",
    "                               'accuracy'+' - ' + label:'mean'\n",
    "                           }).sort_values('F1'+' - ' + label,ascending=False).reset_index()).to_dict('records')[0]\n",
    "        best_model_temp = pd.DataFrame(best_model_temp, index=[modelname])\n",
    "\n",
    "        # take label name out of column names so that everything is general \n",
    "        best_model_temp.columns = [s.replace(label, 'label') for s in list(best_model_temp.columns)] \n",
    "\n",
    "        # add a column name saying the label\n",
    "        simpleLabel = label.replace(modelname + '.', '')\n",
    "        best_model_temp.insert(0, 'label name', simpleLabel) \n",
    "        \n",
    "        # join all together\n",
    "        best_model = pd.concat([best_model, best_model_temp])\n",
    "    \n",
    "    # re-arrange so hyperparameters are at the end\n",
    "    best_model = best_model[[c for c in best_model if c not in params] \n",
    "        + [c for c in params if c in best_model]]\n",
    "    \n",
    "    del inner_scores, inner_df, allLabels, best_model_temp\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0410c4f-92a4-4c53-80b5-726585059b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 macro</th>\n",
       "      <th>F1 micro</th>\n",
       "      <th>F1 weighted</th>\n",
       "      <th>ROC AUC macro</th>\n",
       "      <th>precision macro</th>\n",
       "      <th>recall macro</th>\n",
       "      <th>label name</th>\n",
       "      <th>F1 - label</th>\n",
       "      <th>ROC AUC - label</th>\n",
       "      <th>precision - label</th>\n",
       "      <th>recall - label</th>\n",
       "      <th>accuracy - label</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>class_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adapt_to_threat</th>\n",
       "      <td>0.470982</td>\n",
       "      <td>0.528632</td>\n",
       "      <td>0.542695</td>\n",
       "      <td>0.835704</td>\n",
       "      <td>0.540875</td>\n",
       "      <td>0.494483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>4</td>\n",
       "      <td>{0: 2.116788321167883, 1: 12.774193548387096, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adapt_to_threat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Human</td>\n",
       "      <td>0.786876</td>\n",
       "      <td>0.924077</td>\n",
       "      <td>0.746748</td>\n",
       "      <td>0.833221</td>\n",
       "      <td>0.854892</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adapt_to_threat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natural</td>\n",
       "      <td>0.463664</td>\n",
       "      <td>0.830197</td>\n",
       "      <td>0.371530</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.884352</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>3</td>\n",
       "      <td>{0: 2.116788321167883, 1: 12.774193548387096, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adapt_to_threat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Both</td>\n",
       "      <td>0.335922</td>\n",
       "      <td>0.736343</td>\n",
       "      <td>0.489107</td>\n",
       "      <td>0.268740</td>\n",
       "      <td>0.854521</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>4</td>\n",
       "      <td>{0: 2.116788321167883, 1: 12.774193548387096, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 F1 macro  F1 micro  F1 weighted  ROC AUC macro  \\\n",
       "adapt_to_threat  0.470982  0.528632     0.542695       0.835704   \n",
       "adapt_to_threat       NaN       NaN          NaN            NaN   \n",
       "adapt_to_threat       NaN       NaN          NaN            NaN   \n",
       "adapt_to_threat       NaN       NaN          NaN            NaN   \n",
       "\n",
       "                 precision macro  recall macro label name  F1 - label  \\\n",
       "adapt_to_threat         0.540875      0.494483        NaN         NaN   \n",
       "adapt_to_threat              NaN           NaN      Human    0.786876   \n",
       "adapt_to_threat              NaN           NaN    Natural    0.463664   \n",
       "adapt_to_threat              NaN           NaN       Both    0.335922   \n",
       "\n",
       "                 ROC AUC - label  precision - label  recall - label  \\\n",
       "adapt_to_threat              NaN                NaN             NaN   \n",
       "adapt_to_threat         0.924077           0.746748        0.833221   \n",
       "adapt_to_threat         0.830197           0.371530        0.683333   \n",
       "adapt_to_threat         0.736343           0.489107        0.268740   \n",
       "\n",
       "                 accuracy - label  batch_size  weight_decay  learning_rate  \\\n",
       "adapt_to_threat               NaN          32           0.0        0.00005   \n",
       "adapt_to_threat          0.854892          16           0.0        0.00005   \n",
       "adapt_to_threat          0.884352          32           0.0        0.00005   \n",
       "adapt_to_threat          0.854521          32           0.0        0.00005   \n",
       "\n",
       "                 num_epochs                                       class_weight  \n",
       "adapt_to_threat           4  {0: 2.116788321167883, 1: 12.774193548387096, ...  \n",
       "adapt_to_threat           3                                                 -1  \n",
       "adapt_to_threat           3  {0: 2.116788321167883, 1: 12.774193548387096, ...  \n",
       "adapt_to_threat           4  {0: 2.116788321167883, 1: 12.774193548387096, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2=get_best_model_multi('adapt_to_threat',f'/home/dveytia/ORO-map-relevance/outputs/model_selection/adapt_to_threat_model_selection_', 3)\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671cddcb-43df-4344-bdb3-3236892759a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   label name  F1 macro  F1 micro  F1 weighted  ROC AUC macro  \\\n",
      "adapt_to_threat           NaN  0.470982  0.528632     0.542695       0.835704   \n",
      "adapt_to_threat         Human       NaN       NaN          NaN            NaN   \n",
      "adapt_to_threat       Natural       NaN       NaN          NaN            NaN   \n",
      "adapt_to_threat          Both       NaN       NaN          NaN            NaN   \n",
      "climate_mitigation        NaN       NaN       NaN          NaN            NaN   \n",
      "\n",
      "                    precision macro  recall macro  F1 - label  \\\n",
      "adapt_to_threat            0.540875      0.494483         NaN   \n",
      "adapt_to_threat                 NaN           NaN    0.786876   \n",
      "adapt_to_threat                 NaN           NaN    0.463664   \n",
      "adapt_to_threat                 NaN           NaN    0.335922   \n",
      "climate_mitigation              NaN           NaN         NaN   \n",
      "\n",
      "                    ROC AUC - label  precision - label  ...        F1  \\\n",
      "adapt_to_threat                 NaN                NaN  ...       NaN   \n",
      "adapt_to_threat            0.924077           0.746748  ...       NaN   \n",
      "adapt_to_threat            0.830197           0.371530  ...       NaN   \n",
      "adapt_to_threat            0.736343           0.489107  ...       NaN   \n",
      "climate_mitigation              NaN                NaN  ...  0.802477   \n",
      "\n",
      "                     ROC AUC  precision    recall  accuracy  batch_size  \\\n",
      "adapt_to_threat          NaN        NaN       NaN       NaN          32   \n",
      "adapt_to_threat          NaN        NaN       NaN       NaN          16   \n",
      "adapt_to_threat          NaN        NaN       NaN       NaN          32   \n",
      "adapt_to_threat          NaN        NaN       NaN       NaN          32   \n",
      "climate_mitigation  0.938158   0.726099  0.897321  0.879555          16   \n",
      "\n",
      "                    weight_decay  learning_rate  num_epochs  \\\n",
      "adapt_to_threat              0.0        0.00005           4   \n",
      "adapt_to_threat              0.0        0.00005           3   \n",
      "adapt_to_threat              0.0        0.00005           3   \n",
      "adapt_to_threat              0.0        0.00005           4   \n",
      "climate_mitigation           0.0        0.00001           2   \n",
      "\n",
      "                                                         class_weight  \n",
      "adapt_to_threat     {0: 2.116788321167883, 1: 12.774193548387096, ...  \n",
      "adapt_to_threat                                                    -1  \n",
      "adapt_to_threat     {0: 2.116788321167883, 1: 12.774193548387096, ...  \n",
      "adapt_to_threat     {0: 2.116788321167883, 1: 12.774193548387096, ...  \n",
      "climate_mitigation                                                 -1  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "params = ['batch_size','weight_decay','learning_rate','num_epochs','class_weight']\n",
    "test3 = pd.concat([test2, test1])\n",
    "\n",
    "# arrange params at the end\n",
    "test3 = test3[[c for c in test3 if c not in params] \n",
    "        + [c for c in params if c in test3]] \n",
    "\n",
    "# arrange label name at the beginning\n",
    "temp_cols=test3.columns.tolist()\n",
    "index=test3.columns.get_loc(\"label name\")\n",
    "new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]\n",
    "test3=test3[new_cols]\n",
    "print(test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0084fdd-e63e-421f-b904-253de5641b92",
   "metadata": {},
   "source": [
    "# Get all model scores for all single label models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052aef73-7fd9-4747-a42a-b093dcfcec21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              F1   ROC AUC  precision    recall  accuracy  batch_size  \\\n",
      "screen  0.702171  0.911164   0.663529  0.760859  0.868857          16   \n",
      "\n",
      "        weight_decay  learning_rate  num_epochs  class_weight  \n",
      "screen           0.0        0.00001           4            -1  \n"
     ]
    }
   ],
   "source": [
    "# For the screening model\n",
    "screenModelScores = get_best_model_single('screen',f'/home/dveytia/ORO-map-relevance/outputs/model_selection/screen_model_selection_', 5)\n",
    "\n",
    "print(screenModelScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ff6e7a6-e76b-4de7-a3b6-396111c016d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climate_mitigation\n",
      "Forecast\n",
      "impact_ncp.Any\n",
      "impact_nature\n",
      "blue_carbon\n",
      "biodiversity_metric\n",
      "restoration\n",
      "safe_fish\n",
      "safe_space\n",
      "societal_implemented\n",
      "                          F1   ROC AUC  precision    recall  accuracy  \\\n",
      "climate_mitigation  0.802477  0.938158   0.726099  0.897321  0.879555   \n",
      "Forecast            0.559951  0.849199   0.466964  0.730504  0.863921   \n",
      "\n",
      "                    batch_size  weight_decay  learning_rate  num_epochs  \\\n",
      "climate_mitigation          16           0.0        0.00001           2   \n",
      "Forecast                    32           0.0        0.00005           4   \n",
      "\n",
      "                                    class_weight  \n",
      "climate_mitigation                            -1  \n",
      "Forecast            {0: 1, 1: 7.714285714285714}  \n",
      "                            F1   ROC AUC  precision    recall  accuracy  \\\n",
      "safe_space            0.834123  0.933927   0.768850  0.912925  0.873261   \n",
      "societal_implemented  0.642857  0.583932   0.539927  0.852941  0.533662   \n",
      "\n",
      "                      batch_size  weight_decay  learning_rate  num_epochs  \\\n",
      "safe_space                    16           0.0        0.00005           2   \n",
      "societal_implemented          16           0.0        0.00005           3   \n",
      "\n",
      "                                       class_weight  \n",
      "safe_space            {0: 1, 1: 1.8657718120805369}  \n",
      "societal_implemented  {0: 1, 1: 0.9894736842105263}  \n"
     ]
    }
   ],
   "source": [
    "# create a list of all the single label models to loop through\n",
    "singleModels = ['climate_mitigation','Forecast','impact_ncp.Any','impact_nature','blue_carbon',\n",
    "                'biodiversity_metric', 'restoration', 'safe_fish', 'safe_space','societal_implemented'] \n",
    "\n",
    "for model  in singleModels:\n",
    "    print(model)\n",
    "    temp = get_best_model_single(model,f'/home/dveytia/ORO-map-relevance/outputs/model_selection/{model}_model_selection_', 3)\n",
    "    if model == singleModels[0]:\n",
    "        singleModelScores = temp\n",
    "    else:\n",
    "        singleModelScores = pd.concat([singleModelScores, temp])\n",
    "\n",
    "print(singleModelScores.head(2))\n",
    "print(singleModelScores.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d5453-18e3-42bf-aede-2ce5a38df3a1",
   "metadata": {},
   "source": [
    "# Get model scores for all multi label models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe418a49-f623-4734-85f8-d968656a8b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oro_branch\n",
      "oro_any_mitigation\n",
      "oro_any_nature\n",
      "oro_any_societal\n",
      "data_type\n",
      "adapt_to_threat\n",
      "adapt_to_threat_simplified\n",
      "adapt_to_threat_simplified2\n",
      "climate_threat\n",
      "climate_threat_simplified\n",
      "ecosystem_type\n",
      "ecosystem_type_simplified\n",
      "ecosystem_type_simplified2\n",
      "impact_ncp_nested\n",
      "m_co2_ocean_storage\n",
      "m_co2_removal\n",
      "marine_system\n",
      "method_type\n",
      "method_type_nested\n",
      "method_type_simplified\n",
      "oro_development_stage\n",
      "oro_development_stage_mitigation\n",
      "oro_development_stage_nature\n",
      "oro_development_stage_societal\n",
      "scientific_discipline\n",
      "            F1 macro  F1 micro  F1 weighted  ROC AUC macro  precision macro  \\\n",
      "oro_branch  0.827799  0.863254     0.858565       0.938578         0.851071   \n",
      "oro_branch       NaN       NaN          NaN            NaN              NaN   \n",
      "\n",
      "            recall macro  label name  F1 - label  ROC AUC - label  \\\n",
      "oro_branch      0.816377         NaN         NaN              NaN   \n",
      "oro_branch           NaN  Mitigation    0.932975         0.983207   \n",
      "\n",
      "            precision - label  recall - label  accuracy - label  batch_size  \\\n",
      "oro_branch                NaN             NaN               NaN          16   \n",
      "oro_branch           0.911457        0.955897          0.948435          32   \n",
      "\n",
      "            weight_decay  learning_rate  num_epochs  \\\n",
      "oro_branch           0.0        0.00001           3   \n",
      "oro_branch           0.0        0.00001           4   \n",
      "\n",
      "                                                 class_weight  \n",
      "oro_branch                                                 -1  \n",
      "oro_branch  {0: 1.6855345911949686, 1: 3.5425531914893615,...  \n"
     ]
    }
   ],
   "source": [
    "# create a list of all the multi label models to loop through\n",
    "multiModels = ['oro_branch','oro_any_mitigation','oro_any_nature', 'oro_any_societal',\n",
    "               'data_type',\n",
    "               'adapt_to_threat','adapt_to_threat_simplified','adapt_to_threat_simplified2', \n",
    "               'climate_threat','climate_threat_simplified', \n",
    "               'ecosystem_type','ecosystem_type_simplified','ecosystem_type_simplified2',\n",
    "               'impact_ncp_nested',\n",
    "              'm_co2_ocean_storage', 'm_co2_removal',\n",
    "               'marine_system', \n",
    "               'method_type', 'method_type_nested','method_type_simplified',\n",
    "               'oro_development_stage','oro_development_stage_mitigation','oro_development_stage_nature', 'oro_development_stage_societal',\n",
    "              'scientific_discipline'] # add data\n",
    "\n",
    "for model  in multiModels:\n",
    "    print(model)\n",
    "    temp = get_best_model_multi(model,f'/home/dveytia/ORO-map-relevance/outputs/model_selection/{model}_model_selection_', 3)\n",
    "    if model == multiModels[0]:\n",
    "        multiModelScores = temp\n",
    "    else:\n",
    "        multiModelScores = pd.concat([multiModelScores, temp])\n",
    "\n",
    "print(multiModelScores.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3cbfde-75a0-4f81-b10a-10e821fe7d5a",
   "metadata": {},
   "source": [
    "# Join and write all model scores to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9d75468-84f5-4e4c-9888-7b859951b059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'label name',\n",
       " 'F1',\n",
       " 'ROC AUC',\n",
       " 'precision',\n",
       " 'recall',\n",
       " 'accuracy',\n",
       " 'F1 macro',\n",
       " 'F1 micro',\n",
       " 'F1 weighted',\n",
       " 'ROC AUC macro',\n",
       " 'precision macro',\n",
       " 'recall macro',\n",
       " 'F1 - label',\n",
       " 'ROC AUC - label',\n",
       " 'precision - label',\n",
       " 'recall - label',\n",
       " 'accuracy - label',\n",
       " 'batch_size',\n",
       " 'weight_decay',\n",
       " 'learning_rate',\n",
       " 'num_epochs',\n",
       " 'class_weight']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models_all=pd.concat([screenModelScores, singleModelScores, multiModelScores])\n",
    "\n",
    "# arrange params at the end\n",
    "params = ['batch_size','weight_decay','learning_rate','num_epochs','class_weight']\n",
    "best_models_all = best_models_all[[c for c in best_models_all if c not in params] \n",
    "        + [c for c in params if c in best_models_all]] \n",
    "\n",
    "# arrange label name at the beginning\n",
    "temp_cols=best_models_all.columns.tolist()\n",
    "index=best_models_all.columns.get_loc(\"label name\")\n",
    "new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]\n",
    "best_models_all=best_models_all[new_cols]\n",
    "#best_models_all.head\n",
    "#list(best_models_all.columns)\n",
    "\n",
    "# make the index (model name) its own column and place in position 0\n",
    "best_models_all['model'] = best_models_all.index\n",
    "temp_cols=best_models_all.columns.tolist()\n",
    "index=best_models_all.columns.get_loc(\"model\")\n",
    "new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]\n",
    "best_models_all=best_models_all[new_cols]\n",
    "\n",
    "# check all the column names\n",
    "list(best_models_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5774431-0224-4676-a87d-bf3e8b98a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_all.to_csv(f'/home/dveytia/ORO-map-relevance/outputs/summary_model_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417318b6-078d-4f16-8049-8a41f1147e4e",
   "metadata": {},
   "source": [
    "# No longer needed: Calculate the scores for each label (for multi-label models)\n",
    "\n",
    "In the above functions, for a multi label model only the macro scores are reported. If a dataframe is desired that includes the scores for every label, use the following (note that code is not complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d43317a-3a82-486b-9258-745e2fa583c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_labels(modelname, file_path_prefix, k_range): \n",
    "    inner_scores = []\n",
    "    params = ['batch_size','weight_decay','learning_rate','num_epochs','class_weight']\n",
    "    \n",
    "    for k in range(k_range): \n",
    "        inner_df = pd.read_csv(f'{file_path_prefix}{k}.csv')\n",
    "        inner_df = inner_df.sort_values('F1 macro', ascending=False).reset_index(drop=True)\n",
    "        inner_scores += inner_df.to_dict('records')\n",
    "    \n",
    "    inner_scores = pd.DataFrame.from_dict(inner_scores).fillna(-1)\n",
    "    \n",
    "    if 'accuracy macro' not in list(inner_scores.columns): # if there is no accuracy macro column set to dummy value\n",
    "        inner_scores['accuracy macro'] = -999 \n",
    "    \n",
    "    # Loop across the different labels to get the best scores\n",
    "    \n",
    "    # get the sames of the different labels\n",
    "    allLabels = [x for x in inner_scores.columns if 'F1' in x] \n",
    "    allLabels.remove('F1 macro')\n",
    "    allLabels.remove('F1 micro')\n",
    "    allLabels.remove('F1 weighted')\n",
    "    allLabels.remove('F1 samples')\n",
    "    allLabels = [s.replace('F1 - ', '') for s in allLabels]\n",
    "    \n",
    "    # start loop\n",
    "    for label in allLabels:\n",
    "        \n",
    "        # get the best model\n",
    "        best_model_temp = (inner_scores\n",
    "                  .groupby(params).agg({\n",
    "                      'F1'+' - ' + label:'mean',\n",
    "                      'ROC AUC'+' - ' + label:'mean',\n",
    "                      'precision'+' - ' + label:'mean',\n",
    "                      'recall'+' - ' + label:'mean',\n",
    "                      'accuracy'+' - ' + label:'mean'\n",
    "                      }).sort_values('F1'+' - ' + label,ascending=False).reset_index()).to_dict('records')[0]\n",
    "        best_model_temp = pd.DataFrame(best_model_temp, index=[modelname])\n",
    "\n",
    "        # take label name out of column names so that everything is general \n",
    "        best_model_temp.columns = [s.replace(' - ' + label, '') for s in list(best_model_temp.columns)] \n",
    "\n",
    "        # add a column name saying the label\n",
    "        best_model_temp.insert(0, 'label', label) \n",
    "        \n",
    "        # join all together\n",
    "        if label == allLabels[0]:\n",
    "            labelScores = best_model_temp\n",
    "        else:\n",
    "            labelScores = pd.concat([labelScores, best_model_temp])\n",
    "    del inner_scores, inner_df, allLabels, best_model_temp\n",
    "    return labelScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4622be3a-24f8-4acd-ab13-a54279e86681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_type\n",
      "adapt_to_threat\n",
      "adapt_to_threat_simplified\n",
      "adapt_to_threat_simplified2\n",
      "climate_threat\n",
      "ecosystem_type\n",
      "ecosystem_type_simplified\n",
      "impact_ncp_nested\n",
      "m_co2_ocean_storage\n",
      "m_co2_removal\n",
      "marine_system\n",
      "method_type\n",
      "method_type_nested\n",
      "oro_development_stage\n",
      "oro_development_stage_mitigation\n",
      "oro_development_stage_nature\n",
      "oro_development_stage_societal\n",
      "scientific_discipline\n"
     ]
    }
   ],
   "source": [
    "for model  in multiModels:\n",
    "    print(model)\n",
    "    temp = get_best_model_labels(model,f'/home/dveytia/ORO-map-relevance/outputs/model_selection/{model}_model_selection_', 3)\n",
    "    if model == multiModels[0]:\n",
    "        multiModelLabelScores = temp\n",
    "    else:\n",
    "        multiModelLabelScores = pd.concat([multiModelLabelScores, temp])\n",
    "\n",
    "multiModelLabelScores.to_csv(f'/home/dveytia/ORO-map-relevance/outputs/summary_model_label_scores.csv', index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilBERT_env",
   "language": "python",
   "name": "distilbert_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
