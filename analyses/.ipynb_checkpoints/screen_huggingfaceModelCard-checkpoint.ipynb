{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd226aa-bab5-4efb-97aa-b98532d76715",
   "metadata": {},
   "source": [
    "# Hugging Face model card for DistilBERT uncased text classification model: ORO relevance screening\n",
    "\n",
    "This script will use the model configuration with the hyperparameters determined from the model_selection_excl.py script, and fit using the whole screening dataset. This is different than the model predictions obtained from the nested cross validation script in the 'binary_predictions_excl.py' script which fits models on splits of the data for a distribution of predicitons. This is because a model card can only have one model. So the purpose is just to provide an approximation/example, knowing that the model will likely be overfit compared to the predictions presented in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c1a6e7-4cb5-48e1-b114-7fdbc9e656ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dveytia/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-12 10:00:23.982232: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-12 10:00:24.248520: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-12 10:00:25.419757: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-12 10:00:25.419826: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-12 10:00:25.419833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/miniconda3/envs/distilBERT_env_3.8_modelCard/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load modules\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f3e75-2826-487d-87b3-be4c50b6e1bc",
   "metadata": {},
   "source": [
    "## Get the best model parameters determined from the model selection CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfca07d-64b7-4f96-90ac-886b8b7ac57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which model, tokenizer and parameters that will be used to fit the model\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "outer_scores = []\n",
    "inner_scores = []\n",
    "params = ['batch_size','weight_decay','learning_rate','num_epochs','class_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c42b7e8-f8ef-4eb9-9cab-ce2a1dd0c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 16, 'weight_decay': 0.0, 'learning_rate': 1e-05, 'num_epochs': 4, 'class_weight': -1, 'F1': 0.7021713122907419}\n"
     ]
    }
   ],
   "source": [
    "# From all the folds used for model selection, find the best model\n",
    "for k in range(5): # For all the folds, find the best model\n",
    "    inner_df = pd.read_csv(f'/home/dveytia/ORO-map-relevance/outputs/model_selection/screen_model_selection_{k}.csv') \n",
    "    inner_df = inner_df.sort_values('F1',ascending=False).reset_index(drop=True)\n",
    "    inner_scores += inner_df.to_dict('records')\n",
    "\n",
    "inner_scores = pd.DataFrame.from_dict(inner_scores).fillna(-1)\n",
    "best_model_params = (inner_scores\n",
    "              .groupby(params)['F1']\n",
    "              .mean()\n",
    "              .sort_values(ascending=False)\n",
    "              .reset_index() \n",
    "             ).to_dict('records')[0]\n",
    "\n",
    "\n",
    "\n",
    "# can have a look at the F1 score for the best model\n",
    "print(best_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ee8b6-754b-48c0-8a22-73ad5659a136",
   "metadata": {},
   "source": [
    "## Using the best parameters, fit the model on the full seen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c86ac0-5385-4d15-8395-1ab52f733384",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in and Format the screening data \n",
    "\n",
    "## The 'seen' data\n",
    "seen_df = pd.read_csv('/home/dveytia/ORO-map-relevance/data/seen/all-screen-results_screenExcl-codeIncl.txt', delimiter='\\t')\n",
    "seen_df['seen']=1\n",
    "seen_df = seen_df.rename(columns={'include_screen':'relevant','analysis_id':'id'})\n",
    "seen_df['relevant']=seen_df['relevant'].astype(int)\n",
    "\n",
    "def map_values(x): \n",
    "    value_map = {\n",
    "        \"random\": 1,\n",
    "        \"relevance sort\": 0,\n",
    "        \"test list\": 0,\n",
    "        \"supplemental coding\": 0\n",
    "    }\n",
    "    return value_map.get(x, \"NaN\")\n",
    "\n",
    "seen_df['random_sample'] = seen_df['sample_screen'].apply(map_values)\n",
    "\n",
    "df = seen_df\n",
    "\n",
    "#unseen_df = pd.read_csv('/home/dveytia/ORO-map-relevance/data/unseen/unique_references2.txt', delimiter='\\t')\n",
    "#unseen_df.rename(columns={'analysis_id':'id'}, inplace=True)\n",
    "#unseen_df['seen']=0\n",
    "\n",
    "#nan_count=unseen_df['abstract'].isna().sum()\n",
    "#print('Number of missing abstracts is',nan_count)\n",
    "#nan_articles=unseen_df[unseen_df['abstract'].isna()]\n",
    "#unseen_df=unseen_df.dropna(subset=['abstract']).reset_index(drop=True)\n",
    "\n",
    "#df = (pd.concat([seen_df,unseen_df])\n",
    "#      .sort_values('id')\n",
    "#      .sample(frac=1, random_state=1)\n",
    "#      .reset_index(drop=True)\n",
    "#)\n",
    "\n",
    "\n",
    "#print('Number of unique references WITH abstract is',len(df))\n",
    "\n",
    "#df['text'] = df['title'] + \". \" + df['abstract'] + \" \" + \"Keywords: \" + df[\"keywords\"] \n",
    "# sometimes this line above throws an error, so if it does, run:\n",
    "df['text'] = df['title'].astype(\"str\") + \". \" + df['abstract'].astype(\"str\") + \" \" + \"Keywords: \" + df[\"keywords\"].astype(\"str\") \n",
    "df['text'] = df.apply(lambda row: (row['title'] + \". \" + row['abstract']) if pd.isna(row['text']) else row['text'], axis=1)\n",
    "\n",
    "#seen_index = df[df['seen']==1].index\n",
    "#unseen_index = df[df['seen']==0].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4c1e98-171e-45c9-83ac-f2d11517ced5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████| 669/669 [00:03<00:00, 198.40 examples/s]\n",
      "Map: 100%|██████████████████████████| 2083/2083 [00:10<00:00, 196.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## Convert pandas data frame to Dataset\n",
    "## separate into training (non-randomly sampled) and testing (randomly sampled)\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "train_datasets = Dataset.from_pandas(df.loc[df['random_sample'] == 0, ['text','relevant','random_sample']])\n",
    "eval_datasets = Dataset.from_pandas(df.loc[df['random_sample'] == 1, ['text','relevant','random_sample']])\n",
    "\n",
    "train_tokenized = train_datasets.map(tokenize_function, batched=True)\n",
    "eval_tokenized = eval_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b69ec77-06ee-41bb-911f-1219b06eb533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 10:29:33.731456: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-12 10:29:34.698184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21308 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:3b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Convert Dataset to big tensors and use the tf.data.Dataset.from_tensor_slices method\n",
    "full_train_dataset = train_tokenized\n",
    "full_eval_dataset = eval_tokenized\n",
    "\n",
    "tf_train_dataset = full_train_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
    "train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names}\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset['relevant']))\n",
    "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(best_model_params['batch_size'])\n",
    "\n",
    "tf_eval_dataset = full_eval_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
    "eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names}\n",
    "eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset['relevant']))\n",
    "eval_tf_dataset = eval_tf_dataset.shuffle(len(tf_eval_dataset)).batch(best_model_params['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "073eb295-d60d-4a59-805e-901beb65331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:From /home/dveytia/.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "42/42 [==============================] - 43s 849ms/step - loss: 0.5592 - binary_accuracy: 0.6278 - val_loss: 1.2920 - val_binary_accuracy: 0.2050\n",
      "Epoch 2/4\n",
      "42/42 [==============================] - 34s 829ms/step - loss: 0.4488 - binary_accuracy: 0.7952 - val_loss: 1.3261 - val_binary_accuracy: 0.2612\n",
      "Epoch 3/4\n",
      "42/42 [==============================] - 34s 830ms/step - loss: 0.3743 - binary_accuracy: 0.8281 - val_loss: 1.7808 - val_binary_accuracy: 0.2439\n",
      "Epoch 4/4\n",
      "42/42 [==============================] - 34s 830ms/step - loss: 0.3204 - binary_accuracy: 0.8819 - val_loss: 1.5870 - val_binary_accuracy: 0.3519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.callbacks.History at 0x7f36840f63d0>,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With this, the model can be compiled and trained \n",
    "\n",
    "# define model using best parameters gotten from model selection\n",
    "num_labels = 1 # binary model -- so number of labels = 1\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)  \n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=best_model_params['learning_rate'], weight_decay=best_model_params['weight_decay'])\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# Fit model using training and evaluation datasets\n",
    "model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=best_model_params['num_epochs']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92635a97-5d4d-438d-8360-414a43a12b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf_model.h5: 100%|███████████████████████████| 268M/268M [00:26<00:00, 9.94MB/s]\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\"distilbert_ORO_screen\", use_auth_token = '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')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilBert_modelCard_env",
   "language": "python",
   "name": "distilbert_modelcard_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
