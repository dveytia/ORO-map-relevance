{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd226aa-bab5-4efb-97aa-b98532d76715",
   "metadata": {},
   "source": [
    "# Hugging Face model card for DistilBERT uncased text classification model: ORO relevance screening\n",
    "\n",
    "This script will use the model configuration with the hyperparameters determined from the model_selection_excl.py script, and fit using the whole screening dataset. This is different than the model predictions obtained from the nested cross validation script in the 'binary_predictions_excl.py' script which fits models on splits of the data for a distribution of predicitons. This is because a model card can only have one model. So the purpose is just to provide an approximation/example, knowing that the model will likely be overfit compared to the predictions presented in the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f3e75-2826-487d-87b3-be4c50b6e1bc",
   "metadata": {},
   "source": [
    "## Fit the screening model and push to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c1a6e7-4cb5-48e1-b114-7fdbc9e656ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dveytia/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-12 10:00:23.982232: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-12 10:00:24.248520: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-12 10:00:25.419757: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-12 10:00:25.419826: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-12 10:00:25.419833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/miniconda3/envs/distilBERT_env_3.8_modelCard/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load modules\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfca07d-64b7-4f96-90ac-886b8b7ac57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which model, tokenizer and parameters that will be used to fit the model\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "outer_scores = []\n",
    "inner_scores = []\n",
    "params = ['batch_size','weight_decay','learning_rate','num_epochs','class_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c42b7e8-f8ef-4eb9-9cab-ce2a1dd0c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 16, 'weight_decay': 0.0, 'learning_rate': 1e-05, 'num_epochs': 4, 'class_weight': -1, 'F1': 0.7021713122907419}\n"
     ]
    }
   ],
   "source": [
    "# From all the folds used for model selection, find the best model\n",
    "for k in range(5): # For all the folds, find the best model\n",
    "    inner_df = pd.read_csv(f'/home/dveytia/ORO-map-relevance/outputs/model_selection/screen_model_selection_{k}.csv') \n",
    "    inner_df = inner_df.sort_values('F1',ascending=False).reset_index(drop=True)\n",
    "    inner_scores += inner_df.to_dict('records')\n",
    "\n",
    "inner_scores = pd.DataFrame.from_dict(inner_scores).fillna(-1)\n",
    "best_model_params = (inner_scores\n",
    "              .groupby(params)['F1']\n",
    "              .mean()\n",
    "              .sort_values(ascending=False)\n",
    "              .reset_index() \n",
    "             ).to_dict('records')[0]\n",
    "\n",
    "\n",
    "\n",
    "# can have a look at the F1 score for the best model\n",
    "print(best_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89926b-cd65-46bf-ad7a-da199674a08f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Try using Max's code -- model predictions not too good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc29db5-c9ee-4373-9ee2-81e62749dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(MODEL_NAME, num_labels, params):\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)  \n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd7bdee4-58e1-4349-bfc3-4d052b1060d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "my_model = init_model('distilbert-base-uncased', 1, best_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a2e871e-6e14-4470-8844-49089e24a61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf_model.h5: 100%|███████████████████████████| 268M/268M [00:34<00:00, 7.73MB/s]\n"
     ]
    }
   ],
   "source": [
    "my_model.push_to_hub(\"distilbert_ORO_screen\", use_auth_token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2416fb3b-2fb0-4b8c-b37a-e03e40393451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dveytia/distilbert_ORO_screen/commit/4281423f3d5af853f3d8faa72c1ed3034f9b7ece', commit_message='Upload tokenizer', commit_description='', oid='4281423f3d5af853f3d8faa72c1ed3034f9b7ece', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer.push_to_hub(\"distilbert_ORO_screen\", use_auth_token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eca0be8f-c969-4de0-9b32-653e55ca361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command will create a local directory which you can push manually to git, but it's an extra step\n",
    "#my_model.save_pretrained(\"https://huggingface.co/dveytia/distilbert_ORO_screen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "270c70e4-5995-486d-8aff-456f99f9c505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://huggingface.co/dveytia/distilbert_ORO_screen/tokenizer_config.json',\n",
       " 'https://huggingface.co/dveytia/distilbert_ORO_screen/special_tokens_map.json',\n",
       " 'https://huggingface.co/dveytia/distilbert_ORO_screen/vocab.txt',\n",
       " 'https://huggingface.co/dveytia/distilbert_ORO_screen/added_tokens.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"https://huggingface.co/dveytia/distilbert_ORO_screen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ee8b6-754b-48c0-8a22-73ad5659a136",
   "metadata": {},
   "source": [
    "## Using the best parameters, fit the model on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c86ac0-5385-4d15-8395-1ab52f733384",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in and Format the screening data \n",
    "\n",
    "## The 'seen' data\n",
    "seen_df = pd.read_csv('/home/dveytia/ORO-map-relevance/data/seen/all-screen-results_screenExcl-codeIncl.txt', delimiter='\\t')\n",
    "seen_df['seen']=1\n",
    "seen_df = seen_df.rename(columns={'include_screen':'relevant','analysis_id':'id'})\n",
    "seen_df['relevant']=seen_df['relevant'].astype(int)\n",
    "\n",
    "def map_values(x): \n",
    "    value_map = {\n",
    "        \"random\": 1,\n",
    "        \"relevance sort\": 0,\n",
    "        \"test list\": 0,\n",
    "        \"supplemental coding\": 0\n",
    "    }\n",
    "    return value_map.get(x, \"NaN\")\n",
    "\n",
    "seen_df['random_sample'] = seen_df['sample_screen'].apply(map_values)\n",
    "\n",
    "df = seen_df\n",
    "\n",
    "#unseen_df = pd.read_csv('/home/dveytia/ORO-map-relevance/data/unseen/unique_references2.txt', delimiter='\\t')\n",
    "#unseen_df.rename(columns={'analysis_id':'id'}, inplace=True)\n",
    "#unseen_df['seen']=0\n",
    "\n",
    "#nan_count=unseen_df['abstract'].isna().sum()\n",
    "#print('Number of missing abstracts is',nan_count)\n",
    "#nan_articles=unseen_df[unseen_df['abstract'].isna()]\n",
    "#unseen_df=unseen_df.dropna(subset=['abstract']).reset_index(drop=True)\n",
    "\n",
    "#df = (pd.concat([seen_df,unseen_df])\n",
    "#      .sort_values('id')\n",
    "#      .sample(frac=1, random_state=1)\n",
    "#      .reset_index(drop=True)\n",
    "#)\n",
    "\n",
    "\n",
    "#print('Number of unique references WITH abstract is',len(df))\n",
    "\n",
    "#df['text'] = df['title'] + \". \" + df['abstract'] + \" \" + \"Keywords: \" + df[\"keywords\"] \n",
    "# sometimes this line above throws an error, so if it does, run:\n",
    "df['text'] = df['title'].astype(\"str\") + \". \" + df['abstract'].astype(\"str\") + \" \" + \"Keywords: \" + df[\"keywords\"].astype(\"str\") \n",
    "df['text'] = df.apply(lambda row: (row['title'] + \". \" + row['abstract']) if pd.isna(row['text']) else row['text'], axis=1)\n",
    "\n",
    "#seen_index = df[df['seen']==1].index\n",
    "#unseen_index = df[df['seen']==0].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4c1e98-171e-45c9-83ac-f2d11517ced5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████| 669/669 [00:03<00:00, 198.40 examples/s]\n",
      "Map: 100%|██████████████████████████| 2083/2083 [00:10<00:00, 196.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## Convert pandas data frame to Dataset\n",
    "## separate into training (non-randomly sampled) and testing (randomly sampled)\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "train_datasets = Dataset.from_pandas(df.loc[df['random_sample'] == 0, ['text','relevant','random_sample']])\n",
    "eval_datasets = Dataset.from_pandas(df.loc[df['random_sample'] == 1, ['text','relevant','random_sample']])\n",
    "\n",
    "train_tokenized = train_datasets.map(tokenize_function, batched=True)\n",
    "eval_tokenized = eval_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b69ec77-06ee-41bb-911f-1219b06eb533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 10:29:33.731456: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-12 10:29:34.698184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21308 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:3b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Convert Dataset to big tensors and use the tf.data.Dataset.from_tensor_slices method\n",
    "full_train_dataset = train_tokenized\n",
    "full_eval_dataset = eval_tokenized\n",
    "\n",
    "tf_train_dataset = full_train_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
    "train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names}\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset['relevant']))\n",
    "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(best_model_params['batch_size'])\n",
    "\n",
    "tf_eval_dataset = full_eval_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
    "eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names}\n",
    "eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset['relevant']))\n",
    "eval_tf_dataset = eval_tf_dataset.shuffle(len(tf_eval_dataset)).batch(best_model_params['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "073eb295-d60d-4a59-805e-901beb65331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:From /home/dveytia/.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "42/42 [==============================] - 43s 849ms/step - loss: 0.5592 - binary_accuracy: 0.6278 - val_loss: 1.2920 - val_binary_accuracy: 0.2050\n",
      "Epoch 2/4\n",
      "42/42 [==============================] - 34s 829ms/step - loss: 0.4488 - binary_accuracy: 0.7952 - val_loss: 1.3261 - val_binary_accuracy: 0.2612\n",
      "Epoch 3/4\n",
      "42/42 [==============================] - 34s 830ms/step - loss: 0.3743 - binary_accuracy: 0.8281 - val_loss: 1.7808 - val_binary_accuracy: 0.2439\n",
      "Epoch 4/4\n",
      "42/42 [==============================] - 34s 830ms/step - loss: 0.3204 - binary_accuracy: 0.8819 - val_loss: 1.5870 - val_binary_accuracy: 0.3519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.callbacks.History at 0x7f36840f63d0>,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With this, the model can be compiled and trained \n",
    "\n",
    "# define model using best parameters gotten from model selection\n",
    "num_labels = 1 # binary model -- so number of labels = 1\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)  \n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=best_model_params['learning_rate'], weight_decay=best_model_params['weight_decay'])\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# Fit model using training and evaluation datasets\n",
    "model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=best_model_params['num_epochs']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92635a97-5d4d-438d-8360-414a43a12b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf_model.h5: 100%|███████████████████████████| 268M/268M [00:26<00:00, 9.94MB/s]\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\"distilbert_ORO_screen\", use_auth_token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42859e92-066f-406b-ac9f-95d513fcaea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/dveytia/distilbert_ORO_screen\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "output = query({\n",
    "    \"inputs\": \"The Paris Agreement target of limiting global surface warming to 1.5–2◦C compared to pre-industrial levels by 2100 will still heavily impact the ocean. While ambitious mitigation and adaptation are both needed, the ocean provides major opportunities for action to reduce climate change globally and its impacts on vital ecosystems and ecosystem services. A comprehensive and systematic assessment of 13 global- and local-scale, ocean-based measures was performed to help steer the development and implementation of technologies and actions toward a sustainable outcome. We show that (1) all measures have tradeoffs and multiple criteria must be used for a comprehensive assessment of their potential, (2) greatest benefit is derived by combining global and local solutions, some of which could be implemented or scaled-up immediately, (3) some measures are too uncertain to be recommended yet, (4) political consistency must be achieved through effective cross-scale governance mechanisms, (5) scientific effort must focus on effectiveness, co-benefits, disbenefits, and costs of poorly tested as well as new and emerging measures.\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd380d-cf5a-4e2a-acf4-311278f389d5",
   "metadata": {},
   "source": [
    "# junk code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625ab67-2b34-434d-b8a4-6e57f9473991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed31d4f-a556-4f13-b0eb-90386454fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "#from datasets import load_metric\n",
    "\n",
    "training_args = TrainingArguments(\"test_trainer\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "metric = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11d388-c154-45a0-9d32-498a3dda2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(learning_rate=best_model_params['learning_rate'], weight_decay=best_model_params['weight_decay']),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.BinaryAccuracy(),\n",
    ")\n",
    "\n",
    "model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=),\n",
    "\n",
    "model.fit(train_tf_dataset, \n",
    "          validation_data=eval_tf_dataset,\n",
    "          epochs=best_model_params['num_epochs'],\n",
    "          batch_size=best_model_params['batch_size'],\n",
    "          class_weight=best_model_params['class_weight']\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"https://huggingface.co/dveytia/distilbert_ORO_screen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e4e69-1291-4bcd-8a69-b6bf8c83552e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilBert_modelCard_env",
   "language": "python",
   "name": "distilbert_modelcard_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
