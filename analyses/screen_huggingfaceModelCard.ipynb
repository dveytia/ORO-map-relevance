{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c1a6e7-4cb5-48e1-b114-7fdbc9e656ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dveytia/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-11 12:56:57.335963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 12:56:57.463866: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-11 12:56:58.027036: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-11 12:56:58.027086: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-11 12:56:58.027092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/miniconda3/envs/distilBERT_env_3.8_modelCard/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bfca07d-64b7-4f96-90ac-886b8b7ac57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "outer_scores = []\n",
    "inner_scores = []\n",
    "params = ['batch_size','weight_decay','learning_rate','num_epochs','class_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c42b7e8-f8ef-4eb9-9cab-ce2a1dd0c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 16, 'weight_decay': 0.0, 'learning_rate': 1e-05, 'num_epochs': 4, 'class_weight': '{0: 1, 1: 3.8782201405152223}', 'F1': 0.7225130890052355}\n"
     ]
    }
   ],
   "source": [
    "for k in range(1): # changed from range(5) for test run\n",
    "    inner_df = pd.read_csv(f'/home/dveytia/ORO-map-relevance/outputs/model_selection/screen_model_selection_{k}.csv') \n",
    "    inner_df = inner_df.sort_values('F1',ascending=False).reset_index(drop=True)\n",
    "    inner_scores += inner_df.to_dict('records')\n",
    "\n",
    "inner_scores = pd.DataFrame.from_dict(inner_scores).fillna(-1)\n",
    "best_model_params = (inner_scores\n",
    "              .groupby(params)['F1']\n",
    "              .mean()\n",
    "              .sort_values(ascending=False)\n",
    "              .reset_index() \n",
    "             ).to_dict('records')[0]\n",
    "\n",
    "\n",
    "\n",
    "# can have a look at the F1 score for the best model\n",
    "print(best_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89926b-cd65-46bf-ad7a-da199674a08f",
   "metadata": {},
   "source": [
    "# Try using Max's code -- model predictions not too good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc29db5-c9ee-4373-9ee2-81e62749dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(MODEL_NAME, num_labels, params):\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)  \n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd7bdee4-58e1-4349-bfc3-4d052b1060d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "my_model = init_model('distilbert-base-uncased', 1, best_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a2e871e-6e14-4470-8844-49089e24a61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf_model.h5: 100%|███████████████████████████| 268M/268M [00:34<00:00, 7.73MB/s]\n"
     ]
    }
   ],
   "source": [
    "my_model.push_to_hub(\"distilbert_ORO_screen\", use_auth_token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2416fb3b-2fb0-4b8c-b37a-e03e40393451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dveytia/distilbert_ORO_screen/commit/4281423f3d5af853f3d8faa72c1ed3034f9b7ece', commit_message='Upload tokenizer', commit_description='', oid='4281423f3d5af853f3d8faa72c1ed3034f9b7ece', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer.push_to_hub(\"distilbert_ORO_screen\", use_auth_token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eca0be8f-c969-4de0-9b32-653e55ca361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command will create a local directory which you can push manually to git, but it's an extra step\n",
    "#my_model.save_pretrained(\"https://huggingface.co/dveytia/distilbert_ORO_screen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "270c70e4-5995-486d-8aff-456f99f9c505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://huggingface.co/dveytia/distilbert_ORO_screen/tokenizer_config.json',\n",
       " 'https://huggingface.co/dveytia/distilbert_ORO_screen/special_tokens_map.json',\n",
       " 'https://huggingface.co/dveytia/distilbert_ORO_screen/vocab.txt',\n",
       " 'https://huggingface.co/dveytia/distilbert_ORO_screen/added_tokens.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"https://huggingface.co/dveytia/distilbert_ORO_screen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ee8b6-754b-48c0-8a22-73ad5659a136",
   "metadata": {},
   "source": [
    "# Try different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4860b99c-82d7-4b5c-a53f-252ca5332a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 16,\n",
       " 'weight_decay': 0.0,\n",
       " 'learning_rate': 1e-05,\n",
       " 'num_epochs': 4,\n",
       " 'class_weight': '{0: 1, 1: 3.8782201405152223}',\n",
       " 'F1': 0.7225130890052355}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4c86ac0-5385-4d15-8395-1ab52f733384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1856763/4254989356.py:26: DtypeWarning: Columns (28,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  unseen_df = pd.read_csv('/home/dveytia/ORO-map-relevance/data/unseen/unique_references2.txt', delimiter='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing abstracts is 6\n",
      "Number of unique references WITH abstract is 2776\n"
     ]
    }
   ],
   "source": [
    "# Format data the tensorflow way\n",
    "# Change the name of the documents here:\n",
    "seen_df = pd.read_csv('/home/dveytia/ORO-map-relevance/data/seen/all-screen-results_screenExcl-codeIncl.txt', delimiter='\\t')\n",
    "seen_df['seen']=1\n",
    "seen_df = seen_df.rename(columns={'include_screen':'relevant','analysis_id':'id'})\n",
    "seen_df['relevant']=seen_df['relevant'].astype(int)\n",
    "\n",
    "def map_values(x): \n",
    "    value_map = {\n",
    "        \"random\": 1,\n",
    "        \"relevance sort\": 0,\n",
    "        \"test list\": 0,\n",
    "        \"supplemental coding\": 0\n",
    "    }\n",
    "    return value_map.get(x, \"NaN\")\n",
    "\n",
    "seen_df['random_sample'] = seen_df['sample_screen'].apply(map_values)\n",
    "\n",
    "## add this in to sub-sample dataframe for test run\n",
    "#seen_df1 = seen_df[(seen_df['sample_screen'] == 'random') & (seen_df['relevant'] == 1)]\n",
    "#seen_df1 = seen_df1[0:15]\n",
    "#seen_df2 = seen_df[(seen_df['sample_screen'] == 'random') & (seen_df['relevant'] == 0)]\n",
    "#seen_df2 = seen_df2[0:15]\n",
    "#seen_df = seen_df1.append(seen_df2).reset_index(drop=True)\n",
    "\n",
    "unseen_df = pd.read_csv('/home/dveytia/ORO-map-relevance/data/unseen/unique_references2.txt', delimiter='\\t')\n",
    "# unseen_df = pd.read_csv('C:\\\\Users\\\\vcm20gly\\\\OneDrive - Bangor University\\\\Documents\\\\Review\\\\0_unique_unlabelled_references_VM.csv')\n",
    "unseen_df.rename(columns={'analysis_id':'id'}, inplace=True)\n",
    "unseen_df['seen']=0\n",
    "unseen_df = unseen_df[0:30] # add this to subsample df for test run\n",
    "\n",
    "nan_count=unseen_df['abstract'].isna().sum()\n",
    "print('Number of missing abstracts is',nan_count)\n",
    "nan_articles=unseen_df[unseen_df['abstract'].isna()]\n",
    "unseen_df=unseen_df.dropna(subset=['abstract']).reset_index(drop=True)\n",
    "\n",
    "df = (pd.concat([seen_df,unseen_df])\n",
    "      .sort_values('id')\n",
    "      .sample(frac=1, random_state=1)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print('Number of unique references WITH abstract is',len(df))\n",
    "\n",
    "#df['text'] = df['title'] + \". \" + df['abstract'] + \" \" + \"Keywords: \" + df[\"keywords\"] \n",
    "# sometimes this line above throws an error, so if it does, run:\n",
    "df['text'] = df['title'].astype(\"str\") + \". \" + df['abstract'].astype(\"str\") + \" \" + \"Keywords: \" + df[\"keywords\"].astype(\"str\") \n",
    "df['text'] = df.apply(lambda row: (row['title'] + \". \" + row['abstract']) if pd.isna(row['text']) else row['text'], axis=1)\n",
    "\n",
    "seen_index = df[df['seen']==1].index\n",
    "unseen_index = df[df['seen']==0].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec4c1e98-171e-45c9-83ac-f2d11517ced5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 2752/2752 [00:15<00:00, 182.35 examples/s]\n",
      "Map: 100%|██████████████████████████████| 24/24 [00:00<00:00, 178.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## Convert pandas data frame to Dataset\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "seen_datasets = Dataset.from_pandas(df.loc[df['seen'] == 1, ['text','relevant','seen']])\n",
    "unseen_datasets = Dataset.from_pandas(df.loc[df['seen'] == 0, ['text','relevant','seen']])\n",
    "\n",
    "seen_tokenized = seen_datasets.map(tokenize_function, batched=True)\n",
    "unseen_tokenized = unseen_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b69ec77-06ee-41bb-911f-1219b06eb533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Dataset to big tensors and use the tf.data.Dataset.from_tensor_slices method\n",
    "full_train_dataset = seen_tokenized\n",
    "full_eval_dataset = unseen_tokenized\n",
    "\n",
    "tf_train_dataset = full_train_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
    "train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names}\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset['relevant']))\n",
    "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(best_model_params['batch_size'])\n",
    "\n",
    "tf_eval_dataset = full_eval_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
    "eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names}\n",
    "eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset['relevant']))\n",
    "eval_tf_dataset = eval_tf_dataset.shuffle(len(tf_eval_dataset)).batch(best_model_params['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "073eb295-d60d-4a59-805e-901beb65331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "172/172 [==============================] - 75s 401ms/step - loss: 0.5665 - binary_accuracy: 0.6726 - val_loss: nan - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 2/4\n",
      "172/172 [==============================] - 68s 393ms/step - loss: 0.3827 - binary_accuracy: 0.8299 - val_loss: nan - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 3/4\n",
      "172/172 [==============================] - 67s 391ms/step - loss: 0.2722 - binary_accuracy: 0.8863 - val_loss: nan - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 4/4\n",
      "172/172 [==============================] - 67s 391ms/step - loss: 0.2034 - binary_accuracy: 0.9241 - val_loss: nan - val_binary_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.callbacks.History at 0x7fcfb01d9d00>,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With this, the model can be compiled and trained \n",
    "\n",
    "# define model\n",
    "num_labels = 1\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)  \n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=best_model_params['learning_rate'], weight_decay=best_model_params['weight_decay'])\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=best_model_params['num_epochs']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "903b778b-857e-4ea3-be28-7980ac569f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command will create a local directory which you can push manually to git, but it's an extra step\n",
    "#model.save_pretrained(\"distilbert_ORO_screen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92635a97-5d4d-438d-8360-414a43a12b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf_model.h5: 100%|███████████████████████████| 268M/268M [00:30<00:00, 8.90MB/s]\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\"distilbert_ORO_screen\", use_auth_token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b566373-0466-4209-908a-89eb46e6167a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 16,\n",
       " 'weight_decay': 0.0,\n",
       " 'learning_rate': 1e-05,\n",
       " 'num_epochs': 4,\n",
       " 'class_weight': '{0: 1, 1: 3.8782201405152223}',\n",
       " 'F1': 0.7225130890052355}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42859e92-066f-406b-ac9f-95d513fcaea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/dveytia/distilbert_ORO_screen\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "output = query({\n",
    "    \"inputs\": \"The Paris Agreement target of limiting global surface warming to 1.5–2◦C compared to pre-industrial levels by 2100 will still heavily impact the ocean. While ambitious mitigation and adaptation are both needed, the ocean provides major opportunities for action to reduce climate change globally and its impacts on vital ecosystems and ecosystem services. A comprehensive and systematic assessment of 13 global- and local-scale, ocean-based measures was performed to help steer the development and implementation of technologies and actions toward a sustainable outcome. We show that (1) all measures have tradeoffs and multiple criteria must be used for a comprehensive assessment of their potential, (2) greatest benefit is derived by combining global and local solutions, some of which could be implemented or scaled-up immediately, (3) some measures are too uncertain to be recommended yet, (4) political consistency must be achieved through effective cross-scale governance mechanisms, (5) scientific effort must focus on effectiveness, co-benefits, disbenefits, and costs of poorly tested as well as new and emerging measures.\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd380d-cf5a-4e2a-acf4-311278f389d5",
   "metadata": {},
   "source": [
    "# junk code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625ab67-2b34-434d-b8a4-6e57f9473991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed31d4f-a556-4f13-b0eb-90386454fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "#from datasets import load_metric\n",
    "\n",
    "training_args = TrainingArguments(\"test_trainer\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "metric = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11d388-c154-45a0-9d32-498a3dda2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(learning_rate=best_model_params['learning_rate'], weight_decay=best_model_params['weight_decay']),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.BinaryAccuracy(),\n",
    ")\n",
    "\n",
    "model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=),\n",
    "\n",
    "model.fit(train_tf_dataset, \n",
    "          validation_data=eval_tf_dataset,\n",
    "          epochs=best_model_params['num_epochs'],\n",
    "          batch_size=best_model_params['batch_size'],\n",
    "          class_weight=best_model_params['class_weight']\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"https://huggingface.co/dveytia/distilbert_ORO_screen\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilBert_modelCard_env",
   "language": "python",
   "name": "distilbert_modelcard_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
